---
title: processing raw data to package data
---

```{r include=FALSE}
assertthat::assert_that(
    knitr::current_input() %>% paste0('R-raw/', .) %>% file.exists(),
    msg = 'Please set knitr root directory to project directory! You may try running `rmarkdown::render("data-raw/data.Rmd")` at project directory or clicking `Knit` button on the top of this document in RStudio.'
)
```

```{r clean, include=FALSE}
dir('data', full.names = T) %>% file.remove()
rm(list = ls(envir = globalenv(), all = T))
```

```{r setup, include=FALSE}
internet <- T;
knitr::opts_chunk$set(collapse = T)
```

`open-html R-raw/data.html`

I use two kinds of cache in this document (three if you include `GPL-html/`).

For `dataset`, I use knitr's cache mechanism. Fairly simple, you just need to add 
md5sum of input file to chunk options, kngtr will takes care of the rest for you.  

But remember to use the data in another chunk, otherwise it won't run.

```{r make-dataset, cache=T, input_md5=tools::md5sum('inst/extdata/gds_result.txt')}
dataset <- rGEO::read_summary('inst/extdata/gds_result.txt') %T>% print 
```

```{r use-dataset}
devtools::use_data(dataset, overwrite = T)
```


download .tsv of Platform and Series, click `Homo sapiens` in https://www.ncbi.nlm.nih.gov/geo/browse/?view=platforms


```{r download-tsv-files, eval=internet, cache=T, month=format(Sys.Date(), '%Y-%m')}
# update monthly
geo_tsv <- function(type = c('platforms', 'series'), page = 1, tax_id = '9606') {
    paste0('https://www.ncbi.nlm.nih.gov/geo/browse/?view=', type, '&tax=', tax_id, '&mode=tsv&page=', page, '&display=5000')
}

parallel::mcmapply(download.file, geo_tsv('platforms', 1:2), paste0('data-raw/geo-tsv/platform-', 1:2, '.tsv'))
# parallel::mcmapply(download.file, geo_tsv('series', 1:9),    paste0('data-raw/geo-tsv/series-',   1:9, '.tsv'))
```


```{r platform}
platform <- dir('data-raw/geo-tsv', 'platform', full.names = T) %>% 
	lapply(libzhuoer::read_char_tsv) %>% dplyr::bind_rows() %>% 
	dplyr::filter(Taxonomy == 'Homo sapiens') %T>% print

devtools::use_data(platform, overwrite = T)
```



```{r download-GPL-html}
GPL_html_dir <- 'data-raw/GPL-html';
dir(GPL_html_dir, full = T) %>% {.[file.size(.) <10]} %>% file.remove

download_GPL_html <- function(){
    rGEO.data::platform$Accession %>% 
    {setdiff(., dir(GPL_html_dir) %>% stringr::str_extract('GPL\\d+'))} %>%
    parallel::mclapply(. %>% {
        input  <- paste0('https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=', .);
        output <- paste0(GPL_html_dir, '/', ., '.html');
        if (!file.exists(output)) download.file(input, output)
    }, mc.cores = 16);
}
download_GPL_html()
```

Here I manually maintain a cache. One reason is to save file in case all old 
data get cleaned. Another is that there are two differences, one is old and new 
chunk option, whether input dir mtime or input files; the other is the difference 
between the inut files and our cache. Although they should be the same, there is 
always accidents which we might never understand.

```{r gpl_metas}
cache <- readr::read_rds('data-raw/rds/gpl_metas.rds.gz')

to_process <- setdiff(stringr::str_remove(dir(GPL_html_dir), '.html$'), names(cache))
gpl_html_files <- 

if (length(to_process) > 0) {
    new <- paste0(GPL_html_dir, '/', to_process, '.html') %>% 
        parallel::mclapply(rGEO::read_gpl_html) %T>% {names(.) <- to_process}
    
    gpl_metas <- c(new, cache) %T>% readr::write_rds('data-raw/rds/gpl_metas.rds.gz', 'gz')
} else {
    gpl_metas <- cache
}

devtools::use_data(gpl_metas, overwrite = T)
```




```{r reinstall}
devtools::test()
devtools::document() # you may also have edited data documentation
system('R CMD INSTALL --no-multiarch --with-keep.source .')
devtools::reload()   # now you can use the new data in current R session 
```




